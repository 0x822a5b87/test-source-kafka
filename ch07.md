# 生产者

## 7.1 设计原则

> 1. 生产者通过向broker发送ProducerRequest请求来发送请求；
> 2. 可以通过分区函数将消息路由到特定的分区；
> 3. 生产者内部自动维护到broker的Socket连接；

## 7.2 示例代码

> 一个简单的Partitioner示例。

```scala
public class RoundRobinPartitioner implements Partitioner {

    int i = 0;

    /**
     * 轮询发送数据到partition
     */
    @Override
    public int partition(Object key, int numPartitions) {
        return (++i % numPartitions);
    }
}
```

> A topic, key, and value.
>
> If a partition key is provided it will override the key for the purpose of partitioning but will not be stored.

```scala
case class KeyedMessage[K, V](topic: String, key: K, partKey: Any, message: V) {
  if(topic == null)
    throw new IllegalArgumentException("Topic cannot be null.")
  
  def this(topic: String, message: V) = this(topic, null.asInstanceOf[K], null, message)
  
  def this(topic: String, key: K, message: V) = this(topic, key, key, message)

  // partKey 优先于 key
  def partitionKey: Any = {
    if(partKey != null)
      partKey
    else if(hasKey)
      key
    else
      null  
  }
  
  def hasKey: Boolean = key != null
}
```

## 7.3 模块组成

> 在java代码中调用的 `kafka.javaapi.producer.Producer` 只是对真正的生产者 `kafka.producer.Producer` 的封装。

```scala
class Producer[K,V](private val underlying: kafka.producer.Producer[K,V]) // for testing only
{
  def this(config: ProducerConfig) = this(new kafka.producer.Producer[K,V](config))
  /**
   * Sends the data to a single topic, partitioned by key, using either the
   * synchronous or the asynchronous producer
   * @param message the producer data object that encapsulates the topic, key and message data
   */
  def send(message: KeyedMessage[K,V]): Unit = {
    underlying.send(message)
  }

  /**
   * Use this API to send data to multiple topics
   * @param messages list of producer data objects that encapsulate the topic, key and message data
   */
  def send(messages: java.util.List[KeyedMessage[K,V]]): Unit = {
    import collection.JavaConversions._
    underlying.send((messages: mutable.Buffer[KeyedMessage[K,V]]).toSeq: _*)
  }

  /**
   * Close API to close the producer pool connections to all Kafka brokers. Also closes
   * the zookeeper client connection if one exists
   */
  def close(): Unit = underlying.close()
}
```

> `kafka.producer.Producer` 内部包含了三个主要模块：
>
> - `kafka.producer.async.ProducerSendThread[K,V]` 如果设置为异步发送，则缓存用户发送数据并在 `batch.num.messages` 或者 `queue.buffering.max.ms` 满足条件时发送数据；
> - `kafka.producer.ProducerPool` 缓存客户端和broker的通信；
> - `kafka.producer.async.DefaultEventHandler`  将 KeyedMessage 按照分区规则计算不同的broker应该接收的KeyedMessage，通过ProducerPool内部的SyncProducer发送KeyedMessage。

### 7.3.1 ProducerSendThread

> `kafka.producer.async.ProducerSendThread`

```scala
/**
 * 当producer.type为async时，Producer客户端会启动ProducerSendThread线程，该线程从queue中收集信息并在合适的时机发送数据
 *
 * @param threadName 线程名，"ProducerSendThread-" + config.clientId
 * @param queue      Producer内部的阻塞队列
 * @param handler    处理队列中的batch数据的handler
 * @param queueTime  [[AsyncProducerConfig# queueBufferingMaxMs]]，队列缓存数据的最大时间，默认5秒
 * @param batchSize  [[AsyncProducerConfig# batchNumMessages]]，每次发送的batch数量，默认200
 * @param clientId   clientId
 * @tparam K key
 * @tparam V value
 */
class ProducerSendThread[K, V](val threadName: String,
                               val queue: BlockingQueue[KeyedMessage[K, V]],
                               val handler: EventHandler[K, V],
                               val queueTime: Long,
                               val batchSize: Int,
                               val clientId: String) extends Thread(threadName) with Logging with KafkaMetricsGroup {

  /**
   * shutdownCommand在shutdown()方法被调用时被放到queue中，随后进入[[shutdownLatch.await()]]并等待[[processEvents()]]清空缓存
   * [[processEvents]]方法消费到shutdownCommand会导致[[Stream.continually]]退出，并进入[[tryToHandle()]]方法清空缓存
   * 随后[[processEvents()]]方法退出，进入[[shutdownLatch.countDown()]]
   */
  private val shutdownLatch = new CountDownLatch(1)
  private val shutdownCommand = new KeyedMessage[K, V]("shutdown", null.asInstanceOf[K], null.asInstanceOf[V])

  // ProducerQueueSize metrics
  newGauge("ProducerQueueSize",
    new Gauge[Int] {
      def value: Int = queue.size
    },
    Map("clientId" -> clientId))

  override def run(): Unit = {
    try {
      processEvents()
    } catch {
      case e: Throwable => error("Error in sending events: ", e)
    } finally {
      shutdownLatch.countDown()
    }
  }

  def shutdown(): Unit = {
    info("Begin shutting down ProducerSendThread")
    queue.put(shutdownCommand)
    shutdownLatch.await()
    info("Shutdown ProducerSendThread complete")
  }

  private def processEvents(): Unit = {
    var lastSend = SystemTime.milliseconds
    //queue是调用send时外部感知到的阻塞队列，超时时发送的是events内部的数据
    var events = new ArrayBuffer[KeyedMessage[K, V]]
    var full: Boolean = false

    // drain the queue until you get a shutdown command
    Stream.continually(queue.poll(scala.math.max(0, (lastSend + queueTime) - SystemTime.milliseconds), TimeUnit.MILLISECONDS))
      .takeWhile(item => if (item != null) item ne shutdownCommand else true).foreach {
      currentQueueItem =>
        val elapsed = (SystemTime.milliseconds - lastSend)
        // 如果<currentQueueItem> == null，说明现在是queue.poll超时返回了null
        val expired = currentQueueItem == null
        if (currentQueueItem != null) {
          trace("Dequeued item for topic %s, partition key: %s, data: %s"
            .format(currentQueueItem.topic, currentQueueItem.key, currentQueueItem.message))
          events += currentQueueItem
        }

        // check if the batch size is reached
        full = events.size >= batchSize

        if (full || expired) {
          if (expired)
            debug(elapsed + " ms elapsed. Queue time reached. Sending..")
          if (full)
            debug("Batch full. Sending..")
          // 发送缓存的数据
          tryToHandle(events)
          lastSend = SystemTime.milliseconds
          events = new ArrayBuffer[KeyedMessage[K, V]]
        }
    }
    //收到了<shutdownCommand>，发送剩余的数据
    tryToHandle(events)
    if (queue.size > 0)
      throw new IllegalQueueStateException("Invalid queue state! After queue shutdown, %d remaining items in the queue"
        .format(queue.size))
  }

  /**
   * 发送数据
   * @param events 等待发送的数据
   */
  def tryToHandle(events: Seq[KeyedMessage[K, V]]): Unit = {
    val size = events.size
    try {
      debug("Handling " + size + " events")
      if (size > 0)
        handler.handle(events)
    } catch {
      case e: Throwable => error("Error in handling batch of " + size + " events", e)
    }
  }

}
```

### 7.3.2 ProducerPool

> `ProducerPool` 内部通过 `BrokerId` -> `SyncProducer` 针对每一台包含了目标Partition leader的broker建立了连接。

```scala
object ProducerPool {
  /**
   * 用来创建连接broker的[[SyncProducer]]
   */
  def createSyncProducer(config: ProducerConfig, broker: Broker): SyncProducer = {
    val props = new Properties()
    props.put("host", broker.host)
    props.put("port", broker.port.toString)
    props.putAll(config.props.props)
    new SyncProducer(new SyncProducerConfig(props))
  }
}

class ProducerPool(val config: ProducerConfig) extends Logging {
  /**
   * 保存了所有到broker的[[SyncProducer]]
   */
  private val syncProducers = new mutable.HashMap[Int, SyncProducer]
  private val lock = new Object()

  /**
   * 基于[[TopicMetadata]]更新broker的连接
   * @param topicMetadata [[kafka.api.TopicMetadataResponse.topicsMetadata]]
   */
  def updateProducer(topicMetadata: Seq[TopicMetadata]): Unit = {
    val newBrokers = new collection.mutable.HashSet[Broker]
    topicMetadata.foreach(tmd => {
      tmd.partitionsMetadata.foreach(pmd => {
        /**
         * 1. 可以看到，所有的[[SyncProducer]]都是连接到leader的
         * 2. 对于每一个存在的PartitionMetadata都会建立建立一个连接，但是由于是HashSet[Broker]，所以对于一台机器上
         *    存在多个Partition，只会建立一个连接，这个可以有效的降低连接数。
         */
        if (pmd.leader.isDefined)
          newBrokers += pmd.leader.get
      })
    })
    lock synchronized {
      newBrokers.foreach(b => {
        /**
         * 更新[[syncProducers]]中的[[SyncProducer]]
         */
        if(syncProducers.contains(b.id)){
          syncProducers(b.id).close()
          syncProducers.put(b.id, ProducerPool.createSyncProducer(config, b))
        } else
          syncProducers.put(b.id, ProducerPool.createSyncProducer(config, b))
      })
    }
  }

  def getProducer(brokerId: Int) : SyncProducer = {
    lock.synchronized {
      val producer = syncProducers.get(brokerId)
      producer match {
        case Some(p) => p
        case None => throw new UnavailableProducerException("Sync producer for broker id %d does not exist".format(brokerId))
      }
    }
  }

  /**
   * Closes all the producers in the pool
   */
  def close(): Unit = {
    lock.synchronized {
      info("Closing all sync producers")
      val iter = syncProducers.values.iterator
      while(iter.hasNext)
        iter.next.close()
    }
  }
}
```

#### SyncProducer

> 线程安全，因为所有的 `connect` 和 `send` 操作都使用了 `lock`

```scala
object SyncProducer {
  val RequestKey: Short = 0
  val randomGenerator = new Random
}

/*
 * Send a message set.
 */
@threadsafe
class SyncProducer(val config: SyncProducerConfig) extends Logging {

  private val lock = new Object()
  @volatile private var shutdown: Boolean = false
  /**
   * [[BlockingChannel]]包含了到broker的实际的[[java.net.Socket]]连接
   */
  private val blockingChannel = new BlockingChannel(
    config.host, config.port, BlockingChannel.UseDefaultBufferSize,
    config.sendBufferBytes, config.requestTimeoutMs)
  /**
   * 记录clientId对应的producer发往所有brokers的metrics
   */
  val producerRequestStats: ProducerRequestStats = ProducerRequestStatsRegistry.getProducerRequestStats(config.clientId)

  trace("Instantiating Scala Sync Producer with properties: %s".format(config.props))

  private def verifyRequest(request: RequestOrResponse): Unit = {
    /**
     * This seems a little convoluted, but the idea is to turn on verification simply changing log4j settings
     * Also, when verification is turned on, care should be taken to see that the logs don't fill up with unnecessary
     * data. So, leaving the rest of the logging at TRACE, while errors should be logged at ERROR level
     */
    if (logger.isDebugEnabled) {
      val buffer = new BoundedByteBufferSend(request).buffer
      trace("verifying send buffer of size " + buffer.limit)
      val requestTypeId = buffer.getShort()
      if(requestTypeId == RequestKeys.ProduceKey) {
        val request = ProducerRequest.readFrom(buffer)
        trace(request.toString())
      }
    }
  }

  /**
   * send MetadataRequest和send ProducerRequest都会使用该命令
   */
  private def doSend(request: RequestOrResponse, readResponse: Boolean = true): Receive = {
    lock synchronized {
      // 输出debug日志用于调试
      verifyRequest(request)

      /**
       * 如果[[blockingChannel]]没有连接，则建立连接，建立失败则抛出异常并退出
       */
      getOrMakeConnection()

      var response: Receive = null
      try {
        //发送请求
        blockingChannel.send(request)
        //读取响应
        if(readResponse)
          response = blockingChannel.receive()
        else
          trace("Skipping reading response")
      } catch {
        case e: java.io.IOException =>
          // no way to tell if write succeeded. Disconnect and re-throw exception to let client handle retry
          disconnect()
          throw e
        case e: Throwable => throw e
      }
      response
    }
  }

  /**
   * 发送[[ProducerRequest]]，如果required.request.acks=0，则[[ProducerResponse]]为null
   */
  def send(producerRequest: ProducerRequest): ProducerResponse = {
    val requestSize = producerRequest.sizeInBytes
    //记录metrics
    producerRequestStats.getProducerRequestStats(config.host, config.port).requestSizeHist.update(requestSize)
    producerRequestStats.getProducerRequestAllBrokersStats().requestSizeHist.update(requestSize)

    var response: Receive = null
    val specificTimer = producerRequestStats.getProducerRequestStats(config.host, config.port).requestTimer
    val aggregateTimer = producerRequestStats.getProducerRequestAllBrokersStats().requestTimer
    aggregateTimer.time {
      specificTimer.time {
        //执行实际的发送
        response = doSend(producerRequest, if(producerRequest.requiredAcks == 0) false else true)
      }
    }
    if(producerRequest.requiredAcks != 0)
      ProducerResponse.readFrom(response.buffer)
    else
      null
  }

  def send(request: TopicMetadataRequest): TopicMetadataResponse = {
    val response = doSend(request)
    TopicMetadataResponse.readFrom(response.buffer)
  }

  def close(): Unit = {
    lock synchronized {
      disconnect()
      shutdown = true
    }
  }

  /**
   * Disconnect from current channel, closing connection.
   * Side effect: channel field is set to null on successful disconnect
   */
  private def disconnect(): Unit = {
    try {
      info("Disconnecting from " + formatAddress(config.host, config.port))
      blockingChannel.disconnect()
    } catch {
      case e: Exception => error("Error on disconnect: ", e)
    }
  }

  private def connect(): BlockingChannel = {
    if (!blockingChannel.isConnected && !shutdown) {
      try {
        blockingChannel.connect()
        info("Connected to " + formatAddress(config.host, config.port) + " for producing")
      } catch {
        case e: Exception =>
          disconnect()
          error("Producer connection to " + formatAddress(config.host, config.port) + " unsuccessful", e)
          throw e
      }
    }
    blockingChannel
  }

  private def getOrMakeConnection(): Unit = {
    if(!blockingChannel.isConnected) {
      connect()
    }
  }
}
```

### 7.3.3 DefaultEventHandler

```scala
class DefaultEventHandler[K,V](config: ProducerConfig,
                               private val partitioner: Partitioner,
                               private val encoder: Encoder[V],
                               private val keyEncoder: Encoder[K],
                               private val producerPool: ProducerPool,
                               private val topicPartitionInfos: mutable.HashMap[String, TopicMetadata] = new mutable.HashMap[String, TopicMetadata])
  extends EventHandler[K,V] with Logging {
  //是否同步发送
  val isSync: Boolean = "sync" == config.producerType

  //获取客户端发送消息的correlationId，注意，producer是客户端。
  val correlationId = new AtomicInteger(0)
  val brokerPartitionInfo = new BrokerPartitionInfo(config, producerPool, topicPartitionInfos)

  /**
   * [[TopicMetadata]]的刷新间隔，producer会在两种情况下刷新metadata
   * 1. 查询元数据失败（partition missing， leader not available）
   * 2. 通过此参数间隔定时的刷新
   */
  private val topicMetadataRefreshInterval = config.topicMetadataRefreshIntervalMs
  //metadata最后刷新时间
  private var lastTopicMetadataRefreshTime = 0L
  private val topicMetadataToRefresh = mutable.Set.empty[String]
  //message在未指定partitionKey时会随机的发往一个partition，就缓存在这个partition中
  //该缓存会定期刷新，或者在发送失败时刷新
  //所以如果不指定partitionKey的话，可以发现每一个producer会在某一段时间内写入到某一个特定的partition
  //经过一段时间后，又写入另一个特定的partition
  private val sendPartitionPerTopicCache = mutable.HashMap.empty[String, Int]

  private val producerStats = ProducerStatsRegistry.getProducerStats(config.clientId)
  private val producerTopicStats = ProducerTopicStatsRegistry.getProducerTopicStats(config.clientId)

  def handle(events: Seq[KeyedMessage[K,V]]): Unit = {
    //序列化数据
    val serializedData = serialize(events)
    //metrics
    serializedData.foreach {
      keyed =>
        val dataSize = keyed.message.payloadSize
        producerTopicStats.getProducerTopicStats(keyed.topic).byteRate.mark(dataSize)
        producerTopicStats.getProducerAllTopicsStats().byteRate.mark(dataSize)
    }
    var outstandingProduceRequests = serializedData
    var remainingRetries = config.messageSendMaxRetries + 1
    val correlationIdStart = correlationId.get()
    debug("Handling %d events".format(events.size))
    while (remainingRetries > 0 && outstandingProduceRequests.nonEmpty) {
      //将发送的所有KeyMessage中包含的topic增加到带刷新的元数据中，在合适的时机刷新这些topic对应的元数据
      topicMetadataToRefresh ++= outstandingProduceRequests.map(_.topic)
      if (topicMetadataRefreshInterval >= 0 &&
          SystemTime.milliseconds - lastTopicMetadataRefreshTime > topicMetadataRefreshInterval) {
        Utils.swallowError(brokerPartitionInfo.updateInfo(topicMetadataToRefresh.toSet, correlationId.getAndIncrement))
        sendPartitionPerTopicCache.clear()
        topicMetadataToRefresh.clear
        lastTopicMetadataRefreshTime = SystemTime.milliseconds
      }
      //发送数据
      outstandingProduceRequests = dispatchSerializedData(outstandingProduceRequests)
      //如果返回不为空，说明存在数据发送失败，更新元数据，增加重试次数
      if (outstandingProduceRequests.nonEmpty) {
        info("Back off for %d ms before retrying send. Remaining retries = %d".format(config.retryBackoffMs, remainingRetries-1))
        // back off and update the topic metadata cache before attempting another send operation
        Thread.sleep(config.retryBackoffMs)
        // get topics of the outstanding produce requests and refresh metadata for those
        Utils.swallowError(brokerPartitionInfo.updateInfo(outstandingProduceRequests.map(_.topic).toSet, correlationId.getAndIncrement))
        sendPartitionPerTopicCache.clear()
        remainingRetries -= 1
        producerStats.resendRate.mark()
      }
    }
    //重试次数用完，仍然有数据发送失败，则抛出异常
    if(outstandingProduceRequests.nonEmpty) {
      producerStats.failedSendRate.mark()
      val correlationIdEnd = correlationId.get()
      error("Failed to send requests for topics %s with correlation ids in [%d,%d]"
        .format(outstandingProduceRequests.map(_.topic).toSet.mkString(","),
        correlationIdStart, correlationIdEnd-1))
      throw new FailedToSendMessageException("Failed to send messages after " + config.messageSendMaxRetries + " tries.", null)
    }
  }

  /**
   * 将messages发送到对应的partition，并返回发送失败的messages
   *
   * @param messages 等待发送的messages
   * @return 发送失败的messages
   */
  private def dispatchSerializedData(messages: Seq[KeyedMessage[K,Message]]): Seq[KeyedMessage[K, Message]] = {
    //brokerId -> {[TopicAndPartition] -> Seq[KeyedMessage]}
    val partitionedDataOpt = partitionAndCollate(messages)
    partitionedDataOpt match {
      case Some(partitionedData) =>
        val failedProduceRequests = new ArrayBuffer[KeyedMessage[K,Message]]
        try {
          for ((brokerId, messagesPerBrokerMap) <- partitionedData) {
            //按照压缩相关配置，将Seq[KeyedMessage[K, Message]]转换为[[ByteBufferMessageSet]]
            val messageSetPerBroker = groupMessagesToSet(messagesPerBrokerMap)
            //发送数据到对应的broker，并返回发送失败的partitions
            val failedTopicPartitions = send(brokerId, messageSetPerBroker)
            failedTopicPartitions.foreach(topicPartition => {
              messagesPerBrokerMap.get(topicPartition) match {
                //记录发送失败的消息。
                //TODO
                //注意，这里的data是整个partition的全部消息，需要确认一下
                //kafka的写入机制对于某个partition是否存在事务
                case Some(data) => failedProduceRequests.appendAll(data)
                case None => // nothing
              }
            })
          }
        } catch {
          case t: Throwable => error("Failed to send messages", t)
        }
        failedProduceRequests
      case None =>
        //分区失败，所有的消息都发送失败
        messages
    }
  }

  def serialize(events: Seq[KeyedMessage[K,V]]): Seq[KeyedMessage[K,Message]] = {
    val serializedMessages = new ArrayBuffer[KeyedMessage[K,Message]](events.size)
    events.foreach{e =>
      try {
        if(e.hasKey)
          serializedMessages += new KeyedMessage[K,Message](topic = e.topic, key = e.key, partKey = e.partKey, message = new Message(key = keyEncoder.toBytes(e.key), bytes = encoder.toBytes(e.message)))
        else
          serializedMessages += new KeyedMessage[K,Message](topic = e.topic, key = e.key, partKey = e.partKey, message = new Message(bytes = encoder.toBytes(e.message)))
      } catch {
        case t: Throwable =>
          producerStats.serializationErrorRate.mark()
          if (isSync) {
            throw t
          } else {
            // currently, if in async mode, we just log the serialization error. We need to revisit
            // this when doing kafka-496
            error("Error serializing message for topic %s".format(e.topic), t)
          }
      }
    }
    serializedMessages
  }

  /**
   * 分拣messages，分拣之后的结果是：
   * brokerId -> {[TopicAndPartition] -> Seq[KeyedMessage]}
   * 记得我们前面提到的[[SyncProducer]]是连接到每个broker的，此时我们只需要将结果包含的数据找到对应的[[SyncProducer]]即可发送
   * @param messages 等待发送的消息
   * @return
   */
  def partitionAndCollate(messages: Seq[KeyedMessage[K,Message]]): Option[Map[Int, collection.mutable.Map[TopicAndPartition, Seq[KeyedMessage[K,Message]]]]] = {
    val ret = new mutable.HashMap[Int, collection.mutable.Map[TopicAndPartition, Seq[KeyedMessage[K,Message]]]]
    try {
      for (message <- messages) {
        //获取message对应的Seq[PartitionAndLeader]
        val topicPartitionsList = getPartitionListForTopic(message)
        //根据message.partitionKey来获取对应的partitionId
        val partitionIndex = getPartition(message.topic, message.partitionKey, topicPartitionsList)
        //获取PartitionAndLeader
        val brokerPartition = topicPartitionsList(partitionIndex)

        //将失败推迟到send操作，这样到其他broker的request是正常处理的
        val leaderBrokerId = brokerPartition.leaderBrokerIdOpt.getOrElse(-1)

        //获取发送到发往每个broker的数据，如果ret中不存在就创建它
        var dataPerBroker: mutable.HashMap[TopicAndPartition, Seq[KeyedMessage[K,Message]]] = null
        ret.get(leaderBrokerId) match {
          case Some(element) =>
            dataPerBroker = element.asInstanceOf[mutable.HashMap[TopicAndPartition, Seq[KeyedMessage[K,Message]]]]
          case None =>
            dataPerBroker = new mutable.HashMap[TopicAndPartition, Seq[KeyedMessage[K,Message]]]
            //注意这里，所有不可用的partition都存放在-1这个key下
            ret.put(leaderBrokerId, dataPerBroker)
        }

        val topicAndPartition = TopicAndPartition(message.topic, brokerPartition.partitionId)
        //获取发送到每个broker的数据，并且按照TopicAndPartition分组
        var dataPerTopicPartition: ArrayBuffer[KeyedMessage[K,Message]] = null
        dataPerBroker.get(topicAndPartition) match {
          case Some(element) =>
            dataPerTopicPartition = element.asInstanceOf[ArrayBuffer[KeyedMessage[K,Message]]]
          case None =>
            dataPerTopicPartition = new ArrayBuffer[KeyedMessage[K,Message]]
            dataPerBroker.put(topicAndPartition, dataPerTopicPartition)
        }
        dataPerTopicPartition.append(message)
      }
      Some(ret)
    }catch {    // Swallow recoverable exceptions and return None so that they can be retried.
      case ute: UnknownTopicOrPartitionException => warn("Failed to collate messages by topic,partition due to: " + ute.getMessage); None
      case lnae: LeaderNotAvailableException => warn("Failed to collate messages by topic,partition due to: " + lnae.getMessage); None
      case oe: Throwable => error("Failed to collate messages by topic, partition due to: " + oe.getMessage); None
    }
  }

  private def getPartitionListForTopic(m: KeyedMessage[K,Message]): Seq[PartitionAndLeader] = {
    val topicPartitionsList = brokerPartitionInfo.getBrokerPartitionInfo(m.topic, correlationId.getAndIncrement)
    debug("Broker partitions registered for topic: %s are %s"
      .format(m.topic, topicPartitionsList.map(p => p.partitionId).mkString(",")))
    val totalNumPartitions = topicPartitionsList.length
    if(totalNumPartitions == 0)
      throw new NoBrokersForPartitionException("Partition key = " + m.key)
    topicPartitionsList
  }

  /**
   * 根据topic和[[KeyedMessage.partitionKey]]获取指定的partitionIndex。
   * 注意，这里返回的是在topicPartitionList内的索引，而不是partitionId
   * 在获取的partitionIndex不在0和numPartitions-1内时抛出[[UnknownTopicOrPartitionException]]
   * @param topic The topic
   * @param key the partition key
   * @param topicPartitionList the list of available partitions
   * @return the partition id
   */
  private def getPartition(topic: String, key: Any, topicPartitionList: Seq[PartitionAndLeader]): Int = {
    val numPartitions = topicPartitionList.size
    if(numPartitions <= 0)
      throw new UnknownTopicOrPartitionException("Topic " + topic + " doesn't exist")
    val partition =
      if(key == null) {
        //如果partitionKey为空，我们从缓存中随机选择一个partition
        val id = sendPartitionPerTopicCache.get(topic)
        id match {
          case Some(partitionId) =>
            //直接返回partitionId而不用检查leader可用性
            //因为我们都想将失败推迟到send
            partitionId
          case None =>
            //从leader可用的partition中随机的选择一个partition作为目标地址，并更新缓存
            val availablePartitions = topicPartitionList.filter(_.leaderBrokerIdOpt.isDefined)
            if (availablePartitions.isEmpty)
              throw new LeaderNotAvailableException("No leader for any partition in topic " + topic)
            val index = Utils.abs(Random.nextInt) % availablePartitions.size
            val partitionId = availablePartitions(index).partitionId
            sendPartitionPerTopicCache.put(topic, partitionId)
            partitionId
        }
      } else {
        /**
         * 如果partitionKey不为null，则使用分区函数进行分区
         * [[DefaultPartitioner]]其实就是随机发送到某个分区
         */
        partitioner.partition(key, numPartitions)
      }
    if(partition < 0 || partition >= numPartitions)
      throw new UnknownTopicOrPartitionException("Invalid partition id: " + partition + " for topic " + topic +
        "; Valid values are in the inclusive range of [0, " + (numPartitions-1) + "]")
    trace("Assigning message of topic %s and key %s to a selected partition %d".format(topic, if (key == null) "[none]" else key.toString, partition))
    partition
  }

  /**
   * 基于一个(topic, partition) -> messages的map构造并发送produce request
   *
   * @param brokerId the broker that will receive the request
   * @param messagesPerTopic the messages as a map from (topic, partition) -> messages
   * @return the set (topic, partitions) messages which incurred an error sending or processing
   */
  private def send(brokerId: Int, messagesPerTopic: collection.mutable.Map[TopicAndPartition, ByteBufferMessageSet]) = {
    /**
     * 在[[partitionAndCollate]]中，我们将所有leader不可用的都放在了-1这个key下
     */
    if(brokerId < 0) {
      warn("Failed to send data since partitions %s don't have a leader".format(messagesPerTopic.keys.mkString(",")))
      messagesPerTopic.keys.toSeq
    } else if(messagesPerTopic.nonEmpty) {
      val currentCorrelationId = correlationId.getAndIncrement
      //构造produce request
      val producerRequest = new ProducerRequest(currentCorrelationId, config.clientId, config.requestRequiredAcks,
        config.requestTimeoutMs, messagesPerTopic)
      var failedTopicPartitions = Seq.empty[TopicAndPartition]
      try {
        //从ProducerPool中获取对应的SyncProducer
        val syncProducer = producerPool.getProducer(brokerId)
        //发送消息
        val response = syncProducer.send(producerRequest)
        if(response != null) {
          //request  是 Map[TopicAndPartition, ByteBufferMessageSet]
          //response 是 Map[TopicAndPartition, ProducerResponseStatus]
          //request和response是按照partition一一对应的
          if (response.status.size != producerRequest.data.size) {
            throw new KafkaException("Incomplete response (%s) for producer request (%s)".format(response, producerRequest))
          }
          //找到所有的失败的partition
          val failedPartitionsAndStatus = response.status.filter(_._2.error != ErrorMapping.NoError).toSeq
          failedTopicPartitions = failedPartitionsAndStatus.map(partitionStatus => partitionStatus._1)
          failedTopicPartitions
        } else {
          Seq.empty[TopicAndPartition]
        }
      } catch {
        case t: Throwable =>
          warn("Failed to send producer request with correlation id %d to broker %d with data for partitions %s"
            .format(currentCorrelationId, brokerId, messagesPerTopic.keys.mkString(",")), t)
          messagesPerTopic.keys.toSeq
      }
    } else {
      List.empty
    }
  }

  /**
   * 将按[[TopicAndPartition]]分组后的消息序列转换为[[ByteBufferMessageSet]]
   * @param messagesPerTopicAndPartition 等待转换的消息
   * @return 转换后的[[ByteBufferMessageSet]]
   */
  private def groupMessagesToSet(messagesPerTopicAndPartition: collection.mutable.Map[TopicAndPartition, Seq[KeyedMessage[K, Message]]]) = {
    val messagesPerTopicPartition = messagesPerTopicAndPartition.map { case (topicAndPartition, messages) =>
      //当到达这里时已经确定messages是需要发送到同一个partition了
      val rawMessages = messages.map(_.message)
      (topicAndPartition,
        //不压缩消息
        config.compressionCodec match {
          case NoCompressionCodec =>
            debug("Sending %d messages with no compression to %s".format(messages.size, topicAndPartition))
            new ByteBufferMessageSet(NoCompressionCodec, rawMessages: _*)
          case _ =>
            //如果compressedTopics为空，则对所有的topic开启压缩，否则对特定的topic开启压缩
            config.compressedTopics.size match {
              case 0 =>
                new ByteBufferMessageSet(config.compressionCodec, rawMessages: _*)
              case _ =>
                if (config.compressedTopics.contains(topicAndPartition.topic)) {
                  new ByteBufferMessageSet(config.compressionCodec, rawMessages: _*)
                }
                else {
                  new ByteBufferMessageSet(NoCompressionCodec, rawMessages: _*)
                }
            }
        }
      )
    }
    messagesPerTopicPartition
  }

  def close(): Unit = {
    if (producerPool != null)
      producerPool.close()
  }
}
```

#### BrokerPartitionInfo

```scala
/**
 * 保存了topic->[[TopicMetadata]]的相关信息，提供了查询topic的partition信息的能力
 *
 * @param producerConfig     配置文件，在初始化时通过配置文件解析broker列表，因为查询TopicMetadata需要向随机的broker发送TopicMetadataRequest
 * @param producerPool       [[ProducerPool]]，保存了到broker的[[SyncProducer]]
 * @param topicPartitionInfo topic -> [[TopicMetadata]]，会在某些场景下更新
 */
class BrokerPartitionInfo(producerConfig: ProducerConfig,
                          producerPool: ProducerPool,
                          topicPartitionInfo: mutable.HashMap[String, TopicMetadata])
        extends Logging {
  val brokerList: String = producerConfig.brokerList
  val brokers: Seq[Broker] = ClientUtils.parseBrokerList(brokerList)

  /**
   * 查询某个topic的所有Partition对应的leaderBrokerIdOption
   * @param topic 查询的topic
   * @return Seq[[[kafka.common.TopicAndPartition]] -> leaderBrokerIdOption]，如果没有可用broker返回长度为0的序列
   */
  def getBrokerPartitionInfo(topic: String, correlationId: Int): Seq[PartitionAndLeader] = {
    debug("Getting broker partition info for topic %s".format(topic))
    val topicMetadata = topicPartitionInfo.get(topic)
    val metadata: TopicMetadata =
      topicMetadata match {
        case Some(m) => m
        case None =>
          // 如果没有找到，则通过向broker发送TopicMetadataRequest来更新
          updateInfo(Set(topic), correlationId)
          val topicMetadata = topicPartitionInfo.get(topic)
          topicMetadata match {
            case Some(m) => m
            case None => throw new KafkaException("Failed to fetch topic metadata for topic: " + topic)
          }
      }
    val partitionMetadata = metadata.partitionsMetadata
    //如果没有topic对应的partition信息则抛出异常
    if(partitionMetadata.isEmpty) {
      if(metadata.errorCode != ErrorMapping.NoError) {
        throw new KafkaException(ErrorMapping.exceptionFor(metadata.errorCode))
      } else {
        throw new KafkaException("Topic metadata %s has empty partition metadata and no error code".format(metadata))
      }
    }
    //获取partition对应的leaderIdOption，并按照partitionId从小到大进行排序并返回
    partitionMetadata.map { m =>
      m.leader match {
        case Some(leader) =>
          debug("Partition [%s,%d] has leader %d".format(topic, m.partitionId, leader.id))
          PartitionAndLeader(topic, m.partitionId, Some(leader.id))
        case None =>
          debug("Partition [%s,%d] does not have a leader yet".format(topic, m.partitionId))
          PartitionAndLeader(topic, m.partitionId, None)
      }
    }.sortWith((s, t) => s.partitionId < t.partitionId)
  }

  /**
   * 通过向随机的broker发送[[kafka.javaapi.TopicMetadataRequest]]来更新[[ProducerPool]]内部的缓存
   * @param topics the topics for which the metadata is to be fetched
   */
  def updateInfo(topics: Set[String], correlationId: Int): Unit = {
    var topicsMetadata: Seq[TopicMetadata] = Nil
    val topicMetadataResponse = ClientUtils.fetchTopicMetadata(topics, brokers, producerConfig, correlationId)
    topicsMetadata = topicMetadataResponse.topicsMetadata

    /**
     * 在[[TopicMetadata]]没有错误时，更新[[topicPartitionInfo]]，并在存在错误时输出topic和partition相关的告警日志
     */
    topicsMetadata.foreach(tmd =>{
      trace("Metadata for topic %s is %s".format(tmd.topic, tmd))
      if(tmd.errorCode == ErrorMapping.NoError) {
        topicPartitionInfo.put(tmd.topic, tmd)
      } else {
        warn("Error while fetching metadata [%s] for topic [%s]: %s ".format(tmd, tmd.topic, ErrorMapping.exceptionFor(tmd.errorCode).getClass))
      }
      tmd.partitionsMetadata.foreach(pmd => {
        if (pmd.errorCode != ErrorMapping.NoError && pmd.errorCode == ErrorMapping.LeaderNotAvailableCode) {
          warn("Error while fetching metadata %s for topic partition [%s,%d]: [%s]".format(pmd, tmd.topic, pmd.partitionId,
            ErrorMapping.exceptionFor(pmd.errorCode).getClass))
        } // any other error code (e.g. ReplicaNotAvailable) can be ignored since the producer does not need to access the replica and isr metadata
      })
    })
    producerPool.updateProducer(topicsMetadata)
  }
  
}

case class PartitionAndLeader(topic: String, partitionId: Int, leaderBrokerIdOpt: Option[Int])
```

## 7.4 发送模式

> 发送分为`sync`和`async`两种模式。

### 7.4.1 sync

> 在sync模式下，有三个场景需要更新metadata：
>
> 1. 根据配置的时间间隔定时更新；
> 2. 发送过程中需要对message进行分组，如果发现某个topic的metadata不存在；
> 3. 发送失败，进入重试流程，重试之前会更新元数据；

### 7.4.2 aysnc

> async模式和sync模式的区别是，async模式将消息添加到了`kafka.producer.Producer#queue`下，由 ProducerSendThread来发送消息。
>
> **最终发送消息都是由`kafka.producer.async.EventHandler`发送消息**

```scala
  /**
   * Sends the data, partitioned by key to the topic using either the
   * synchronous or the asynchronous producer
   * @param messages the producer data object that encapsulates the topic, key and message data
   */
  def send(messages: KeyedMessage[K,V]*): Unit = {
    lock synchronized {
      if (hasShutdown.get)
        throw new ProducerClosedException
      recordStats(messages)
      if (sync) {
        eventHandler.handle(messages)
      } else {
        asyncSend(messages)
      }
    }
  }

  private def asyncSend(messages: Seq[KeyedMessage[K,V]]): Unit = {
    for (message <- messages) {
      val added = config.queueEnqueueTimeoutMs match {
        // 0: events will be enqueued immediately or dropped if the queue is full
        case 0  =>
          queue.offer(message)
        case _  =>
          try {
            if (config.queueEnqueueTimeoutMs < 0) {
              queue.put(message)
              true
            } else {
              queue.offer(message, config.queueEnqueueTimeoutMs, TimeUnit.MILLISECONDS)
            }
          }
          catch {
            case e: InterruptedException =>
              false
          }
      }
      if(!added) {
        producerTopicStats.getProducerTopicStats(message.topic).droppedMessageRate.mark()
        producerTopicStats.getProducerAllTopicsStats().droppedMessageRate.mark()
        throw new QueueFullException("Event queue is full of unsent messages, could not send event: " + message.toString)
      }else {
        trace("Added to send queue an event: " + message.toString)
        trace("Remaining queue size: " + queue.remainingCapacity)
      }
    }
  }
```

































