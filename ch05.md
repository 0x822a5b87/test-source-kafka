# broker 的控制管理模块

> 本章介绍kafka的controller
>
> 1. controller 选举
> 2. 初始化
> 3. 维护分区状态和副本状态的机制
> 4. 内部的监听器
> 5. 负载均衡机制
> 6. topic 删除机制
> 7. 内部的通信原理

## 5.1 KafkaController选举

> controller 的选举比较简单，每个broker在 startup 中都会启动 `KafkaController`
>
> ```scala
>   def startup() {
>     // ...
>     kafkaController = new KafkaController(config, zkClient, brokerState)
>     // ...
>   }
> ```
>
> 在启动的时候，会到 zookeeper 的 `/controller` 路径尝试注册一个 `ephemeral node`，注册成功的 broker 则成为整个集群的唯一一台 `leader controller`，而其他注册失败的 broker 则开始监听这个 ephemeral node 的状态，当 leader controller 掉线时，重新发起上述流程选举出新的 leader controller。
>
> ```bash
> get /controller
> # {"version":1,"brokerid":0,"timestamp":"1652877672721"}
> ```

### LeaderElector

```scala
/**
 * This trait defines a leader elector If the existing leader is dead, this class will handle automatic
 * re-election and if it succeeds, it invokes the leader state change callback
 */
trait LeaderElector extends Logging {
  def startup

  def amILeader : Boolean

  def elect: Boolean

  def close
}
```

### ZookeeperLeaderElector

> ZookeeperLeaderElector 的所有参数：
>
> - controllerContext 为 Controller 上下文，包含了 topic 的元数据以及集群的元数据信息；
> - electionPath 是路径，默认为 `/controller`；
> - onBecomingLeader 是选举成为 leader 时的回调函数；
> - onResigningAsLeader 当 leader controller 变成非 leader 时的回调函数，当成为非 leader 的时候我们需要清理内部的数据结构；
> - brokerId 是当前 brokerId；

```scala
/**
 * This class handles zookeeper based leader election based on an ephemeral path. The election module does not handle
 * session expiration, instead it assumes the caller will handle it by probably try to re-elect again. If the existing
 * leader is dead, this class will handle automatic re-election and if it succeeds, it invokes the leader state change
 * callback
 */
class ZookeeperLeaderElector(controllerContext: ControllerContext,
                             electionPath: String,
                             onBecomingLeader: () => Unit,
                             onResigningAsLeader: () => Unit,
                             brokerId: Int)
  extends LeaderElector with Logging {
  // 初始化 leaderId
  var leaderId: Int = -1
  // create the election path in ZK, if one does not exist
  val index: Int = electionPath.lastIndexOf("/")
  if (index > 0)
    makeSurePersistentPathExists(controllerContext.zkClient, electionPath.substring(0, index))
  // leader 变化时的回调函数，包含了两个回调函数
  // handleDataChange  : 当 electionPath 路径上的节点数据被修改时，修改 leaderId；
  // handleDataDeleted : 当 electionPath 路径上的节点数据被删除（说明leader失效），进入leader选举流程。
  val leaderChangeListener = new LeaderChangeListener

  /**
   * startup 在 KafkaController#startup 中调用：
   * 1. 首先在 electionPath 上注册回调函数；
   * 2. 随后进入选举流程。
   */
  def startup: Unit = {
    inLock(controllerContext.controllerLock) {
      controllerContext.zkClient.subscribeDataChanges(electionPath, leaderChangeListener)
      elect
    }
  }

  /**
   * 从electionPath读取leader相关信息，如果已经有broker注册为leader返回leaderId，否则返回 -1
   * @return
   */
  private def getControllerID: Int = {
    readDataMaybeNull(controllerContext.zkClient, electionPath)._1 match {
       case Some(controller) => KafkaController.parseControllerId(controller)
       case None => -1
    }
  }

  /**
   * controller 选举流程
   * @return 成为leader返回true，否则返回false
   */
  def elect: Boolean = {
    val timestamp = SystemTime.milliseconds.toString
    val electString = Json.encode(Map("version" -> 1, "brokerId" -> brokerId, "timestamp" -> timestamp))
   
   leaderId = getControllerID
    /* 
     * 我们可能在startup或者handleDeleted ZK回调函数中参与选举，由于潜在的竞争条件，当我们到达这里时，controller 选举可能已经结束了。
     * 如果此代理已经是控制器，则此检查将防止以下 createEphemeralPath 方法进入无限循环。
     */
    if(leaderId != -1) {
       debug("Broker %d has been elected as leader, so stopping the election process.".format(leaderId))
       return amILeader
    }

    // 尝试将自身信息注册到 electionPath 节点，如果成功则当选leader，失败则进入leader竞选失败流程，作为follower初始化leader相关信息。
    try {
      // 在 electionPath 上尝试选举leader（创建 ephemeral node 并且写入数据 electString）
      // 如果当前 electionPath 对应的值为 brokerId 的话会重试
      createEphemeralPathExpectConflictHandleZKBug(controllerContext.zkClient, electionPath, electString, brokerId,
        (controllerString : String, leaderId : Any) => KafkaController.parseControllerId(controllerString) == leaderId.asInstanceOf[Int],
        controllerContext.zkSessionTimeout)
      info(brokerId + " successfully elected as leader")
      // 修改 leaderId
      leaderId = brokerId
      // 进入回调函数
      onBecomingLeader()
    } catch {
      case e: ZkNodeExistsException =>
        // If someone else has written the path, then
        leaderId = getControllerID 

        if (leaderId != -1)
          debug("Broker %d was elected as leader instead of broker %d".format(leaderId, brokerId))
        else
          warn("A leader has been elected but just resigned, this will result in another round of election")

      case e2: Throwable =>
        error("Error while electing or becoming leader on broker %d".format(brokerId), e2)
        // 如果在选举中碰到其他异常，需要 resign
        resign()
    }
    amILeader
  }

  def close: Unit = {
    leaderId = -1
  }

  def amILeader : Boolean = leaderId == brokerId

  def resign(): Boolean = {
    leaderId = -1
    deletePath(controllerContext.zkClient, electionPath)
  }

  /**
   * We do not have session expiration listen in the ZkElection, but assuming the caller who uses this module will
   * have its own session expiration listener and handler
   */
  class LeaderChangeListener extends IZkDataListener with Logging {
    /**
     * Called when the leader information stored in zookeeper has changed. Record the new leader in memory
     * @throws Exception On any error.
     */
    @throws(classOf[Exception])
    def handleDataChange(dataPath: String, data: Object): Unit = {
      inLock(controllerContext.controllerLock) {
        leaderId = KafkaController.parseControllerId(data.toString)
        info("New leader is %d".format(leaderId))
      }
    }

    /**
     * Called when the leader information stored in zookeeper has been delete. Try to elect as the leader
     * @throws Exception
     *             On any error.
     */
    @throws(classOf[Exception])
    def handleDataDeleted(dataPath: String): Unit = {
      inLock(controllerContext.controllerLock) {
        debug("%s leader change listener fired for path %s to handle data deleted: trying to elect as a leader"
          .format(brokerId, dataPath))
        if(amILeader)
          onResigningAsLeader()
        elect
      }
    }
  }
}
```

## 5.2 KafkaController的初始化

> controller 的初始化由 `onBecomingLeader` 和 `onResigningAsLeader` 两个回调函数来执行，而他们的实际实现是 `onControllerFailover` 和 `onControllerResignation` 。
>
> ```scala
>   private val controllerElector = new ZookeeperLeaderElector(controllerContext, ZkUtils.ControllerPath, onControllerFailover,
>     onControllerResignation, config.brokerId)
> ```

### Leader状态下的KafkaController初始化

> KafkaController#onControllerFailover 方法是初始化 leader 的实际方法，主要流程包括：
>
> 1. 初始化集群内的系统时钟，也就是 controller epoch；
> 2. 在zookeeper的各个path上注册各种监听函数，响应 topic创建、topic删除等各种事件；
> 3. 初始化controller上下文，即集群内的元数据信息，比如 liveBrokers 等；
> 4. 初始化并启动Replica状态机，Partition状态机；
> 5. 切换状态为 `RunningAsController`
> 6. 处理集群初始化之前用户下发的 `PartitionReassignment` 和 `PreferredReplicaElection` 请求；
> 7. 同步元数据给集群的 controller follower；
> 8. 根据配置启动负载均衡线程、topic 删除线程。

```scala
  /**
   * This callback is invoked by the zookeeper leader elector on electing the current broker as the new controller.
   * It does the following things on the become-controller state change -
   * 1. Register controller epoch changed listener
   * 2. Increments the controller epoch
   * 3. Initializes the controller's context object that holds cache objects for current topics, live brokers and
   *    leaders for all existing partitions.
   * 4. Starts the controller's channel manager
   * 5. Starts the replica state machine
   * 6. Starts the partition state machine
   * If it encounters any unexpected exception/error while becoming controller, it resigns as the current controller.
   * This ensures another controller election will be triggered and there will always be an actively serving controller
   */
  def onControllerFailover(): Unit = {
    // 判断controller的状态，当 KafkaServer#shutdown 调用时，会设置 isRunning = false
    if(isRunning) {
      info("Broker %d starting become controller state transition".format(config.brokerId))
      // 从zk中读取epoch和epochZkVersion到ControllerContext
      readControllerEpochFromZookeeper()
      // 更新zk中的epoch
      incrementControllerEpoch(zkClient)
      // 在 /admin/reassign_partitions 注册监听器，监听 partition 重新分配的事件
      registerReassignedPartitionsListener()
      // 在 /admin/preferred_replica_election 注册监听器，监听 replica 选举事件
      registerPreferredReplicaElectionListener()
      // 初始化 partition 状态机并注册监听器
      partitionStateMachine.registerListeners()
      // 初始化 replica 状态机并注册监听器
      replicaStateMachine.registerListeners()
      // 初始化ControllerContext
      initializeControllerContext()
      // 启动 replica 状态机
      replicaStateMachine.startup()
      // 启动 partition 状态机
      partitionStateMachine.startup()
      // register the partition change listeners for all existing topics on failover
      controllerContext.allTopics.foreach(topic => partitionStateMachine.registerPartitionChangeListener(topic))
      info("Broker %d is ready to serve as the new controller with epoch %d".format(config.brokerId, epoch))
      brokerState.newState(RunningAsController)
      // 处理集群初始化之前用户下发的 `PartitionReassignment` 和 `PreferredReplicaElection` 请求；
      maybeTriggerPartitionReassignment()
      maybeTriggerPreferredReplicaElection()
      /* send partition leadership info to all live brokers */
      sendUpdateMetadataRequest(controllerContext.liveOrShuttingDownBrokerIds.toSeq)
      // 启动kafka负载均衡机制
      if (config.autoLeaderRebalanceEnable) {
        info("starting the partition rebalance scheduler")
        autoRebalanceScheduler.startup()
        autoRebalanceScheduler.schedule("partition-rebalance-thread", checkAndTriggerPartitionRebalance,
          5, config.leaderImbalanceCheckIntervalSeconds, TimeUnit.SECONDS)
      }
      // 启动TopicDeletionManager
      deleteTopicManager.start()
    }
    else
      info("Controller has been shut down, aborting startup/failover")
  }
```

### Standby状态下的KafkaController初始化

> KafkaController#onControllerResignation 方法是初始化follower的实际方法，主要流程包括：
>
> 1. 取消针对目录 `/admin/reassign_partitions` 和 `/admin/preferred_replica_election` 的监听，这个由 controller 负责；
> 2. 关闭topic删除线程和负载均衡线程；
> 3. 取消针对于 `/broker/topics/[topic]/[partition]/state` 的监听；
> 4. 关闭并清除Replica状态机，Partition状态机；
> 5. 关闭和其他 KafkaController 的通信链路，因为只有controller向broker发送请求，broker只能被动等待controller的请求；
> 6. 重置集群内部时钟；
> 7. 切换状态为 RunningAsBroker；

```scala
  /**
   * This callback is invoked by the zookeeper leader elector when the current broker resigns as the controller. This is
   * required to clean up internal controller data structures
   */
  def onControllerResignation(): Unit = {
    // de-register listeners
    deregisterReassignedPartitionsListener()
    deregisterPreferredReplicaElectionListener()

    // shutdown delete topic manager
    if (deleteTopicManager != null)
      deleteTopicManager.shutdown()

    // shutdown leader rebalance scheduler
    if (config.autoLeaderRebalanceEnable)
      autoRebalanceScheduler.shutdown()

    inLock(controllerContext.controllerLock) {
      // de-register partition ISR listener for on-going partition reassignment task
      deregisterReassignedPartitionsIsrChangeListeners()
      // shutdown partition state machine
      partitionStateMachine.shutdown()
      // shutdown replica state machine
      replicaStateMachine.shutdown()
      // shutdown controller channel manager
      if(controllerContext.controllerChannelManager != null) {
        controllerContext.controllerChannelManager.shutdown()
        controllerContext.controllerChannelManager = null
      }
      // reset controller context
      controllerContext.epoch=0
      controllerContext.epochZkVersion=0
      brokerState.newState(RunningAsBroker)
    }
  }
```

## 5.3 Topic 的分区状态转换机制

> topic 的分区状态由 `PartitionStateMachine` 模块负责，通过监听 `/brokers/topics` 和 `admin/delete_topics` 来实现监听topic的创建与删除。

### 5.3.1 分区状态的分类

> This class represents the state machine for partitions. It defines the states that a partition can be in, and transitions to move the partition to another legal state. The different states that a partition can be in are - 
>
> 1. `NonExistentPartition`: This state indicates that the partition was either never created or was created and then deleted. Valid previous state, if one exists, is OfflinePartition 
> 2. `NewPartition` : After creation, the partition is in the NewPartition state. In this state, the partition should have replicas assigned to it, but no leader/isr yet. Valid previous states are NonExistentPartition
> 3. `OnlinePartition` : Once a leader is elected for a partition, it is in the OnlinePartition state. Valid previous states are NewPartition/OfflinePartition
> 4. `OfflinePartition` : If, after successful leader election, the leader for partition dies, then the partition moves to the OfflinePartition state. Valid previous states are NewPartition/OnlinePartition

```scala
class PartitionStateMachine(controller: KafkaController) extends Logging {
    private val partitionState: mutable.Map[TopicAndPartition, PartitionState] = mutable.Map.empty
}
```

### 5.3.2 分区状态的转换

```scala
  /**
   * This API exercises the partition's state machine. It ensures that every state transition happens from a legal
   * previous state to the target state. Valid state transitions are:
   * NonExistentPartition -> NewPartition:
   * --load assigned replicas from ZK to controller cache
   *
   * NewPartition -> OnlinePartition
   * --assign first live replica as the leader and all live replicas as the isr; write leader and isr to ZK for this partition
   * --send LeaderAndIsr request to every live replica and UpdateMetadata request to every live broker
   *
   * OnlinePartition,OfflinePartition -> OnlinePartition
   * --select new leader and isr for this partition and a set of replicas to receive the LeaderAndIsr request, and write leader and isr to ZK
   * --for this partition, send LeaderAndIsr request to every receiving replica and UpdateMetadata request to every live broker
   *
   * NewPartition,OnlinePartition,OfflinePartition -> OfflinePartition
   * --nothing other than marking partition state as Offline
   *
   * OfflinePartition -> NonExistentPartition
   * --nothing other than marking the partition state as NonExistentPartition
   * @param topic       The topic of the partition for which the state transition is invoked
   * @param partition   The partition for which the state transition is invoked
   * @param targetState The end state that the partition should be moved to
   */
  private def handleStateChange(topic: String, partition: Int, targetState: PartitionState,
                                leaderSelector: PartitionLeaderSelector,
                                callbacks: Callbacks): Unit = {

  }
```

> 下面的图简单说明了一些状态转换，但是不包含全部的状态转换。

![PartitionStateMachine.drawio.png](source/PartitionStateMachine.drawio.png)

> publilc 函数，这个方法可以看到在哪些情况下我们会发生partition的状态变换：
>
> - KafkaController#shutdownBroker
> - KafkaController#onBrokerFailure
> - KafkaController#onNewPartitionCreation
> - KafkaController#onPreferredReplicaElection
> - KafkaController#moveReassignedPartitionLeaderIfRequired
> - TopicDeletionManager#completeDeleteTopic

```scala
  /**
   * This API is invoked by the partition change zookeeper listener
   * @param partitions   The list of partitions that need to be transitioned to the target state
   * @param targetState  The state that the partitions should be moved to
   */
  def handleStateChanges(partitions: Set[TopicAndPartition], targetState: PartitionState,
                         leaderSelector: PartitionLeaderSelector = noOpPartitionLeaderSelector,
                         callbacks: Callbacks = (new CallbackBuilder).build): Unit = {
    info("Invoking state change to %s for partitions %s".format(targetState, partitions.mkString(",")))
    try {
      brokerRequestBatch.newBatch()
      partitions.foreach { topicAndPartition =>
        handleStateChange(topicAndPartition.topic, topicAndPartition.partition, targetState, leaderSelector, callbacks)
      }
      brokerRequestBatch.sendRequestsToBrokers(controller.epoch, controllerContext.correlationId.getAndIncrement)
    }catch {
      case e: Throwable => error("Error while moving some partitions to %s state".format(targetState), e)
      // TODO: It is not enough to bail out and log an error, it is important to trigger state changes for those partitions
    }
  }
```

#### 5.3.2.1 NonExistentPartition -> NewPartition

> NonExistenPartition转换到NewPartition比较简单，仅仅是持久化topic在zk目录上的各个分区AR列表到KafkaController内存，然后置分区状态为`NewPartition`。

```scala
      targetState match {
        case NewPartition =>
          // 确认前置状态是否合法
          assertValidPreviousStates(topicAndPartition, List(NonExistentPartition), NewPartition)
          // 读取 zk 中 /brokers/topics/[topic] 目录下持久化的 <TopicAndPartition> -> Replicas 并缓存到 controllerContext.partitionReplicaAssignment
          // 这个是在 topic 创建的时候生成的。
          assignReplicasToPartitions(topic, partition)
          partitionState.put(topicAndPartition, NewPartition)
          val assignedReplicas = controllerContext.partitionReplicaAssignment(topicAndPartition).mkString(",")
          stateChangeLogger.trace("Controller %d epoch %d changed partition %s state from %s to %s with assigned replicas %s"
                                    .format(controllerId, controller.epoch, topicAndPartition, currState, targetState,
                                            assignedReplicas))
          // post: partition has been assigned replicas
      }
```

> 数据需要缓存到 KafkaController 中

```scala
  /**
   * Invoked on the NonExistentPartition->NewPartition state transition to update the controller's cache with the
   * partition's replica assignment.
   * @param topic     The topic of the partition whose replica assignment is to be cached
   * @param partition The partition whose replica assignment is to be cached
   */
  private def assignReplicasToPartitions(topic: String, partition: Int): Unit = {
    val assignedReplicas = ZkUtils.getReplicasForPartition(controllerContext.zkClient, topic, partition)
    controllerContext.partitionReplicaAssignment += TopicAndPartition(topic, partition) -> assignedReplicas
  }
```

#### 5.3.2.2 NewPartition、OnlinePartition、OfflinePartition -> OnlinePartition

> 状态转换：
>
> - **NewPartition -> OnlinePartition**利用分区的AR列表初始化leader和ISR
> - **OnlinePartition -> OnlinePartition** 利用 `PartitionLeaderSelector` 选举leader并初始化ISR
> - **OfflinePartition -> OnlinePartition** 利用 `PartitionLeaderSelector` 选举leader并初始化ISR
>
> **NewPartition -> OnlinePartition** 不需要选举leader，因为此时topic刚创建，所有的replica都是没有数据的，我们只需要取任意一个replica作为leader，此时其他的replica自动成为ISR，便完成了 OnlinePartition 初始化。
>
> **OfflinePartition -> OnlinePartition**主要发生在所有的replica下线然后有部分replica上线时，此时 leader 和 ISR的选择需要考虑初始的ISR、AR以及当前liveBrokers之间的关系：优先选择ISR里的第一个replica，否则选择AR里的第一个liveBroker；
>
> **OnlinePartition -> OnlinePartition**主要发生在当前集群出现leader负载不均衡的情况，可能某一台broker的负载非常高，我们希望先将某一个replica的leader迁移到负载较低的机器；
>
> **由于不同的场景下，**

```scala
        case OnlinePartition =>
          // 不能直接从 NonExistentPartition 转换为 OnlinePartition
          assertValidPreviousStates(topicAndPartition, List(NewPartition, OnlinePartition, OfflinePartition), OnlinePartition)
          partitionState(topicAndPartition) match {
            case NewPartition =>
              // 如果前置状态是 NewPartition，我们无需进行选举
              // 因为此时所有的replica都是ISR，并且可以从ISR中任意的选择一个replica作为leader
              // initialize leader and isr path for new partition
              initializeLeaderAndIsrForPartition(topicAndPartition)
            case OfflinePartition =>
              // 如果前置状态是 OfflinePartition 或者 OnlinePartition 则必须进行选举来得到合适的leader
              electLeaderForPartition(topic, partition, leaderSelector)
            case OnlinePartition => // invoked when the leader needs to be re-elected
              electLeaderForPartition(topic, partition, leaderSelector)
            case _ => // should never come here since illegal previous states are checked above
          }
          // 修改partitionState为 OnlinePartition
          partitionState.put(topicAndPartition, OnlinePartition)
```

##### PartitionStateMachine#initializeLeaderAndIsrForPartition

> `PartitionStateMachine#initializeLeaderAndIsrForPartition` Invoked on the NewPartition->OnlinePartition state change. When a partition is in the New state, it does not have a leader and isr path in zookeeper. Once the partition moves to the OnlinePartition state, it's leader and isr path gets initialized and it never goes back to the NewPartition state. From here, it can only go to the OfflinePartition state.
>
> 这里会给 broker 下发 `LeaderAndIsrRequest` ，broker 接收到请求之后，broker 会进入 `becomeLeader` 或者 `becomeFollower` 的流程。

```scala
  private def initializeLeaderAndIsrForPartition(topicAndPartition: TopicAndPartition): Unit = {
    // 获取<TopicAndPartition> 对应 Seq<Replica>
    val replicaAssignment = controllerContext.partitionReplicaAssignment(topicAndPartition)
    // 获取存活的 AR
    val liveAssignedReplicas = replicaAssignment.filter(r => controllerContext.liveBrokerIds.contains(r))
    liveAssignedReplicas.size match {
      // 0 说明没有存活的AR
      case 0 =>
        val failMsg = ("encountered error during state change of partition %s from New to Online, assigned replicas are [%s], " +
                       "live brokers are [%s]. No assigned replica is alive.")
                         .format(topicAndPartition, replicaAssignment.mkString(","), controllerContext.liveBrokerIds)
        stateChangeLogger.error("Controller %d epoch %d ".format(controllerId, controller.epoch) + failMsg)
        throw new StateChangeFailedException(failMsg)
      case _ =>
        debug("Live assigned replicas for partition %s are: [%s]".format(topicAndPartition, liveAssignedReplicas))
        // 选择第一个AR作为leader
        val leader = liveAssignedReplicas.head
        val leaderIsrAndControllerEpoch = new LeaderIsrAndControllerEpoch(new LeaderAndIsr(leader, liveAssignedReplicas.toList),
          controller.epoch)
        debug("Initializing leader and isr for partition %s to %s".format(topicAndPartition, leaderIsrAndControllerEpoch))
        try {
          // 持久化PartitionState的节点信息，类似于：
          // /brokers/topics/test/partitions/0/state -> {"controller_epoch":1,"leader":0,"version":1,"leader_epoch":0,"isr":[0]}
          ZkUtils.createPersistentPath(controllerContext.zkClient,
            ZkUtils.getTopicPartitionLeaderAndIsrPath(topicAndPartition.topic, topicAndPartition.partition),
            ZkUtils.leaderAndIsrZkData(leaderIsrAndControllerEpoch.leaderAndIsr, controller.epoch))
          // NOTE: the above write can fail only if the current controller lost its zk session and the new controller
          // took over and initialized this partition. This can happen if the current controller went into a long
          // GC pause
          // 更新 KafkaController 内存
          controllerContext.partitionLeadershipInfo.put(topicAndPartition, leaderIsrAndControllerEpoch)
          // 增加 LeaderAndIsrRequest 和 UpdateMetadataRequest 下发到 broker
          brokerRequestBatch.addLeaderAndIsrRequestForBrokers(liveAssignedReplicas, topicAndPartition.topic,
            topicAndPartition.partition, leaderIsrAndControllerEpoch, replicaAssignment)
        } catch {
          case e: ZkNodeExistsException =>
            // read the controller epoch
            val leaderIsrAndEpoch = ReplicationUtils.getLeaderIsrAndEpochForPartition(zkClient, topicAndPartition.topic,
              topicAndPartition.partition).get
            val failMsg = ("encountered error while changing partition %s's state from New to Online since LeaderAndIsr path already " +
                           "exists with value %s and controller epoch %d")
                             .format(topicAndPartition, leaderIsrAndEpoch.leaderAndIsr.toString(), leaderIsrAndEpoch.controllerEpoch)
            stateChangeLogger.error("Controller %d epoch %d ".format(controllerId, controller.epoch) + failMsg)
            throw new StateChangeFailedException(failMsg)
        }
    }
  }
```

##### PartitionStateMachine#electLeaderForPartition

> Invoked on the OfflinePartition,OnlinePartition->OnlinePartition state change. It invokes the leader election API to elect a leader for the input offline partition
>
> 需要注意的是，`initializeLeaderAndIsrForPartition` 和 `electLeaderForPartition` 整体的流程都是一致的：
>
> 1. 选举得出最新的 leader，ISR；
> 2. 下发 LeaderAndIsrRequest 和 UpdateMedataRequest 到下游 broker；

```scala
  def electLeaderForPartition(topic: String, partition: Int, leaderSelector: PartitionLeaderSelector): Unit = {
    val topicAndPartition = TopicAndPartition(topic, partition)
    // handle leader election for the partitions whose leader is no longer alive
    stateChangeLogger.trace("Controller %d epoch %d started leader election for partition %s"
                              .format(controllerId, controller.epoch, topicAndPartition))
    try {
      var zookeeperPathUpdateSucceeded: Boolean = false
      var newLeaderAndIsr: LeaderAndIsr = null
      var replicasForThisPartition: Seq[Int] = Seq.empty[Int]
      while(!zookeeperPathUpdateSucceeded) {
        // 从 /broker/topics/[topic]/partitions/[partition]/state 读取 LeaderIsrAndControllerEpoch
        val currentLeaderIsrAndEpoch = getLeaderIsrAndEpochOrThrowException(topic, partition)
        val currentLeaderAndIsr = currentLeaderIsrAndEpoch.leaderAndIsr
        val controllerEpoch = currentLeaderIsrAndEpoch.controllerEpoch
        // 如果当前zk的epoch大于controller的epoch，说明当前的controller可能已经失效并选举除了新的controller
        // 新的controller已经修改了zk的状态
        if (controllerEpoch > controller.epoch) {
          val failMsg = ("aborted leader election for partition [%s,%d] since the LeaderAndIsr path was " +
                         "already written by another controller. This probably means that the current controller %d went through " +
                         "a soft failure and another controller was elected with epoch %d.")
                           .format(topic, partition, controllerId, controllerEpoch)
          stateChangeLogger.error("Controller %d epoch %d ".format(controllerId, controller.epoch) + failMsg)
          throw new StateChangeFailedException(failMsg)
        }
        // 选举出一个新的leader，这里是一个接口，因为不同的情况有不同的选举策略。
        val (leaderAndIsr, replicas) = leaderSelector.selectLeader(topicAndPartition, currentLeaderAndIsr)
        // 更新 /broker/topics/[topic]/partitions/[partition]/state 中分区状态元数据
        val (updateSucceeded, newVersion) = ReplicationUtils.updateLeaderAndIsr(zkClient, topic, partition,
          leaderAndIsr, controller.epoch, currentLeaderAndIsr.zkVersion)
        newLeaderAndIsr = leaderAndIsr
        newLeaderAndIsr.zkVersion = newVersion
        zookeeperPathUpdateSucceeded = updateSucceeded
        replicasForThisPartition = replicas
      }
      // 选举完之后的流程和 initializeLeaderAndIsrForPartition 是一样的
      // 1. 修改 KafkaController 缓存的 partitionLeadershipInfo
      // 2. 发送 LeaderIsrRequest 和 UpdateMedataRequest 到 broker
      val newLeaderIsrAndControllerEpoch = new LeaderIsrAndControllerEpoch(newLeaderAndIsr, controller.epoch)
      // update the leader cache
      controllerContext.partitionLeadershipInfo.put(TopicAndPartition(topic, partition), newLeaderIsrAndControllerEpoch)
      stateChangeLogger.trace("Controller %d epoch %d elected leader %d for Offline partition %s"
                                .format(controllerId, controller.epoch, newLeaderAndIsr.leader, topicAndPartition))
      val replicas = controllerContext.partitionReplicaAssignment(TopicAndPartition(topic, partition))
      // store new leader and isr info in cache
      brokerRequestBatch.addLeaderAndIsrRequestForBrokers(replicasForThisPartition, topic, partition,
        newLeaderIsrAndControllerEpoch, replicas)
    } catch {
      case lenne: LeaderElectionNotNeededException => // swallow
      case nroe: NoReplicaOnlineException => throw nroe
      case sce: Throwable =>
        val failMsg = "encountered error while electing leader for partition %s due to: %s.".format(topicAndPartition, sce.getMessage)
        stateChangeLogger.error("Controller %d epoch %d ".format(controllerId, controller.epoch) + failMsg)
        throw new StateChangeFailedException(failMsg, sce)
    }
    debug("After leader election, leader cache is updated to %s".format(controllerContext.partitionLeadershipInfo.map(l => (l._1, l._2))))
  }
```

#### 5.3.2.3 NewPartition、OnlinePartition、OfflinePartition -> OfflinePartition

> 仅需要在 KafkaController 中修改partition状态为 OfflinePartition

```scala
        case OfflinePartition =>
          // pre: partition should be in New or Online state
          assertValidPreviousStates(topicAndPartition, List(NewPartition, OnlinePartition, OfflinePartition), OfflinePartition)
          // should be called when the leader for a partition is no longer alive
          stateChangeLogger.trace("Controller %d epoch %d changed partition %s state from %s to %s"
                                    .format(controllerId, controller.epoch, topicAndPartition, currState, targetState))
          partitionState.put(topicAndPartition, OfflinePartition)
```

#### 5.3.2.4 OfflinePartition -> NonExistentPartition

> 仅需要在 KafkaController 中修改partition状态为 NonExistentPartition

```scala
        case NonExistentPartition =>
          // pre: partition should be in Offline state
          assertValidPreviousStates(topicAndPartition, List(OfflinePartition), NonExistentPartition)
          stateChangeLogger.trace("Controller %d epoch %d changed partition %s state from %s to %s"
                                    .format(controllerId, controller.epoch, topicAndPartition, currState, targetState))
          partitionState.put(topicAndPartition, NonExistentPartition)
```

### 5.3.3 PartitionStateMachine 模块的启动

































