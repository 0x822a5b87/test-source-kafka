# broker 的控制管理模块

> 本章介绍kafka的controller
>
> 1. controller 选举
> 2. 初始化
> 3. 维护分区状态和副本状态的机制
> 4. 内部的监听器
> 5. 负载均衡机制
> 6. topic 删除机制
> 7. 内部的通信原理

## 5.1 KafkaController选举

> controller 的选举比较简单，每个broker在 startup 中都会启动 `KafkaController`
>
> ```scala
>   def startup() {
>     // ...
>     kafkaController = new KafkaController(config, zkClient, brokerState)
>     // ...
>   }
> ```
>
> 在启动的时候，会到 zookeeper 的 `/controller` 路径尝试注册一个 `ephemeral node`，注册成功的 broker 则成为整个集群的唯一一台 `leader controller`，而其他注册失败的 broker 则开始监听这个 ephemeral node 的状态，当 leader controller 掉线时，重新发起上述流程选举出新的 leader controller。
>
> ```bash
> get /controller
> # {"version":1,"brokerid":0,"timestamp":"1652877672721"}
> ```

### LeaderElector

```scala
/**
 * This trait defines a leader elector If the existing leader is dead, this class will handle automatic
 * re-election and if it succeeds, it invokes the leader state change callback
 */
trait LeaderElector extends Logging {
  def startup

  def amILeader : Boolean

  def elect: Boolean

  def close
}
```

### ZookeeperLeaderElector

> ZookeeperLeaderElector 的所有参数：
>
> - controllerContext 为 Controller 上下文，包含了 topic 的元数据以及集群的元数据信息；
> - electionPath 是路径，默认为 `/controller`；
> - onBecomingLeader 是选举成为 leader 时的回调函数；
> - onResigningAsLeader 当 leader controller 变成非 leader 时的回调函数，当成为非 leader 的时候我们需要清理内部的数据结构；
> - brokerId 是当前 brokerId；

```scala
/**
 * This class handles zookeeper based leader election based on an ephemeral path. The election module does not handle
 * session expiration, instead it assumes the caller will handle it by probably try to re-elect again. If the existing
 * leader is dead, this class will handle automatic re-election and if it succeeds, it invokes the leader state change
 * callback
 */
class ZookeeperLeaderElector(controllerContext: ControllerContext,
                             electionPath: String,
                             onBecomingLeader: () => Unit,
                             onResigningAsLeader: () => Unit,
                             brokerId: Int)
  extends LeaderElector with Logging {
  // 初始化 leaderId
  var leaderId: Int = -1
  // create the election path in ZK, if one does not exist
  val index: Int = electionPath.lastIndexOf("/")
  if (index > 0)
    makeSurePersistentPathExists(controllerContext.zkClient, electionPath.substring(0, index))
  // leader 变化时的回调函数，包含了两个回调函数
  // handleDataChange  : 当 electionPath 路径上的节点数据被修改时，修改 leaderId；
  // handleDataDeleted : 当 electionPath 路径上的节点数据被删除（说明leader失效），进入leader选举流程。
  val leaderChangeListener = new LeaderChangeListener

  /**
   * startup 在 KafkaController#startup 中调用：
   * 1. 首先在 electionPath 上注册回调函数；
   * 2. 随后进入选举流程。
   */
  def startup: Unit = {
    inLock(controllerContext.controllerLock) {
      controllerContext.zkClient.subscribeDataChanges(electionPath, leaderChangeListener)
      elect
    }
  }

  /**
   * 从electionPath读取leader相关信息，如果已经有broker注册为leader返回leaderId，否则返回 -1
   * @return
   */
  private def getControllerID: Int = {
    readDataMaybeNull(controllerContext.zkClient, electionPath)._1 match {
       case Some(controller) => KafkaController.parseControllerId(controller)
       case None => -1
    }
  }

  /**
   * controller 选举流程
   * @return 成为leader返回true，否则返回false
   */
  def elect: Boolean = {
    val timestamp = SystemTime.milliseconds.toString
    val electString = Json.encode(Map("version" -> 1, "brokerId" -> brokerId, "timestamp" -> timestamp))
   
   leaderId = getControllerID
    /* 
     * 我们可能在startup或者handleDeleted ZK回调函数中参与选举，由于潜在的竞争条件，当我们到达这里时，controller 选举可能已经结束了。
     * 如果此代理已经是控制器，则此检查将防止以下 createEphemeralPath 方法进入无限循环。
     */
    if(leaderId != -1) {
       debug("Broker %d has been elected as leader, so stopping the election process.".format(leaderId))
       return amILeader
    }

    // 尝试将自身信息注册到 electionPath 节点，如果成功则当选leader，失败则进入leader竞选失败流程，作为follower初始化leader相关信息。
    try {
      // 在 electionPath 上尝试选举leader（创建 ephemeral node 并且写入数据 electString）
      // 如果当前 electionPath 对应的值为 brokerId 的话会重试
      createEphemeralPathExpectConflictHandleZKBug(controllerContext.zkClient, electionPath, electString, brokerId,
        (controllerString : String, leaderId : Any) => KafkaController.parseControllerId(controllerString) == leaderId.asInstanceOf[Int],
        controllerContext.zkSessionTimeout)
      info(brokerId + " successfully elected as leader")
      // 修改 leaderId
      leaderId = brokerId
      // 进入回调函数
      onBecomingLeader()
    } catch {
      case e: ZkNodeExistsException =>
        // If someone else has written the path, then
        leaderId = getControllerID 

        if (leaderId != -1)
          debug("Broker %d was elected as leader instead of broker %d".format(leaderId, brokerId))
        else
          warn("A leader has been elected but just resigned, this will result in another round of election")

      case e2: Throwable =>
        error("Error while electing or becoming leader on broker %d".format(brokerId), e2)
        // 如果在选举中碰到其他异常，需要 resign
        resign()
    }
    amILeader
  }

  def close: Unit = {
    leaderId = -1
  }

  def amILeader : Boolean = leaderId == brokerId

  def resign(): Boolean = {
    leaderId = -1
    deletePath(controllerContext.zkClient, electionPath)
  }

  /**
   * We do not have session expiration listen in the ZkElection, but assuming the caller who uses this module will
   * have its own session expiration listener and handler
   */
  class LeaderChangeListener extends IZkDataListener with Logging {
    /**
     * Called when the leader information stored in zookeeper has changed. Record the new leader in memory
     * @throws Exception On any error.
     */
    @throws(classOf[Exception])
    def handleDataChange(dataPath: String, data: Object): Unit = {
      inLock(controllerContext.controllerLock) {
        leaderId = KafkaController.parseControllerId(data.toString)
        info("New leader is %d".format(leaderId))
      }
    }

    /**
     * Called when the leader information stored in zookeeper has been delete. Try to elect as the leader
     * @throws Exception
     *             On any error.
     */
    @throws(classOf[Exception])
    def handleDataDeleted(dataPath: String): Unit = {
      inLock(controllerContext.controllerLock) {
        debug("%s leader change listener fired for path %s to handle data deleted: trying to elect as a leader"
          .format(brokerId, dataPath))
        if(amILeader)
          onResigningAsLeader()
        elect
      }
    }
  }
}
```

## 5.2 KafkaController的初始化

> controller 的初始化由 `onBecomingLeader` 和 `onResigningAsLeader` 两个回调函数来执行，而他们的实际实现是 `onControllerFailover` 和 `onControllerResignation` 。
>
> ```scala
>   private val controllerElector = new ZookeeperLeaderElector(controllerContext, ZkUtils.ControllerPath, onControllerFailover,
>     onControllerResignation, config.brokerId)
> ```

### Leader状态下的KafkaController初始化

> KafkaController#onControllerFailover 方法是初始化 leader 的实际方法，主要流程包括：
>
> 1. 初始化集群内的系统时钟，也就是 controller epoch；
> 2. 在zookeeper的各个path上注册各种监听函数，响应 topic创建、topic删除等各种事件；
> 3. 初始化controller上下文，即集群内的元数据信息，比如 liveBrokers 等；
> 4. 初始化并启动Replica状态机，Partition状态机；
> 5. 切换状态为 `RunningAsController`
> 6. 处理集群初始化之前用户下发的 `PartitionReassignment` 和 `PreferredReplicaElection` 请求；
> 7. 同步元数据给集群的 controller follower；
> 8. 根据配置启动负载均衡线程、topic 删除线程。

```scala
  /**
   * This callback is invoked by the zookeeper leader elector on electing the current broker as the new controller.
   * It does the following things on the become-controller state change -
   * 1. Register controller epoch changed listener
   * 2. Increments the controller epoch
   * 3. Initializes the controller's context object that holds cache objects for current topics, live brokers and
   *    leaders for all existing partitions.
   * 4. Starts the controller's channel manager
   * 5. Starts the replica state machine
   * 6. Starts the partition state machine
   * If it encounters any unexpected exception/error while becoming controller, it resigns as the current controller.
   * This ensures another controller election will be triggered and there will always be an actively serving controller
   */
  def onControllerFailover(): Unit = {
    // 判断controller的状态，当 KafkaServer#shutdown 调用时，会设置 isRunning = false
    if(isRunning) {
      info("Broker %d starting become controller state transition".format(config.brokerId))
      // 从zk中读取epoch和epochZkVersion到ControllerContext
      readControllerEpochFromZookeeper()
      // 更新zk中的epoch
      incrementControllerEpoch(zkClient)
      // 在 /admin/reassign_partitions 注册监听器，监听 partition 重新分配的事件
      registerReassignedPartitionsListener()
      // 在 /admin/preferred_replica_election 注册监听器，监听 replica 选举事件
      registerPreferredReplicaElectionListener()
      // 初始化 partition 状态机并注册监听器
      partitionStateMachine.registerListeners()
      // 初始化 replica 状态机并注册监听器
      replicaStateMachine.registerListeners()
      // 初始化ControllerContext
      initializeControllerContext()
      // 启动 replica 状态机
      replicaStateMachine.startup()
      // 启动 partition 状态机
      partitionStateMachine.startup()
      // register the partition change listeners for all existing topics on failover
      controllerContext.allTopics.foreach(topic => partitionStateMachine.registerPartitionChangeListener(topic))
      info("Broker %d is ready to serve as the new controller with epoch %d".format(config.brokerId, epoch))
      brokerState.newState(RunningAsController)
      // 处理集群初始化之前用户下发的 `PartitionReassignment` 和 `PreferredReplicaElection` 请求；
      maybeTriggerPartitionReassignment()
      maybeTriggerPreferredReplicaElection()
      /* send partition leadership info to all live brokers */
      sendUpdateMetadataRequest(controllerContext.liveOrShuttingDownBrokerIds.toSeq)
      // 启动kafka负载均衡机制
      if (config.autoLeaderRebalanceEnable) {
        info("starting the partition rebalance scheduler")
        autoRebalanceScheduler.startup()
        autoRebalanceScheduler.schedule("partition-rebalance-thread", checkAndTriggerPartitionRebalance,
          5, config.leaderImbalanceCheckIntervalSeconds, TimeUnit.SECONDS)
      }
      // 启动TopicDeletionManager
      deleteTopicManager.start()
    }
    else
      info("Controller has been shut down, aborting startup/failover")
  }
```

### Standby状态下的KafkaController初始化

> KafkaController#onControllerResignation 方法是初始化follower的实际方法，主要流程包括：
>
> 1. 取消针对目录 `/admin/reassign_partitions` 和 `/admin/preferred_replica_election` 的监听，这个由 controller 负责；
> 2. 关闭topic删除线程和负载均衡线程；
> 3. 取消针对于 `/broker/topics/[topic]/[partition]/state` 的监听；
> 4. 关闭并清除Replica状态机，Partition状态机；
> 5. 关闭和其他 KafkaController 的通信链路，因为只有controller向broker发送请求，broker只能被动等待controller的请求；
> 6. 重置集群内部时钟；
> 7. 切换状态为 RunningAsBroker；

```scala
  /**
   * This callback is invoked by the zookeeper leader elector when the current broker resigns as the controller. This is
   * required to clean up internal controller data structures
   */
  def onControllerResignation(): Unit = {
    // de-register listeners
    deregisterReassignedPartitionsListener()
    deregisterPreferredReplicaElectionListener()

    // shutdown delete topic manager
    if (deleteTopicManager != null)
      deleteTopicManager.shutdown()

    // shutdown leader rebalance scheduler
    if (config.autoLeaderRebalanceEnable)
      autoRebalanceScheduler.shutdown()

    inLock(controllerContext.controllerLock) {
      // de-register partition ISR listener for on-going partition reassignment task
      deregisterReassignedPartitionsIsrChangeListeners()
      // shutdown partition state machine
      partitionStateMachine.shutdown()
      // shutdown replica state machine
      replicaStateMachine.shutdown()
      // shutdown controller channel manager
      if(controllerContext.controllerChannelManager != null) {
        controllerContext.controllerChannelManager.shutdown()
        controllerContext.controllerChannelManager = null
      }
      // reset controller context
      controllerContext.epoch=0
      controllerContext.epochZkVersion=0
      brokerState.newState(RunningAsBroker)
    }
  }
```



































